{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x, y, dataset):\n",
    "    pairs = list(zip(x, y))\n",
    "    np.random.shuffle(pairs)\n",
    "    x_shuffled = tuple([x[0] for x in pairs])\n",
    "    y_shuffled = tuple([x[1] for x in pairs])\n",
    "    \n",
    "    if dataset == 'MNIST':\n",
    "        x_shuffled = np.array(x_shuffled).reshape(-1, 784)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        x_shuffled = np.array(x_shuffled).reshape(-1, 3, 32, 32)\n",
    "    else:\n",
    "        print('dataset parameter should be MNIST or CIFAR10')\n",
    "        return\n",
    "    mean_x = np.mean(x_shuffled, axis=0).reshape(1, -1)\n",
    "    std_x = np.std(x_shuffled)\n",
    "\n",
    "    x_shuffled = (x_shuffled - mean_x) / std_x\n",
    "    return x_shuffled, y_shuffled\n",
    "\n",
    "def reshaper(torch_dataset, dataset):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for x, y in torch_dataset:\n",
    "        if dataset == 'MNIST':\n",
    "            xs.append(np.array(x, dtype=np.float32).reshape(1, -1))\n",
    "        else:\n",
    "            xs.append(np.array(x, dtype=np.float32).reshape(3, 32, 32))\n",
    "        try:\n",
    "            ys.append(y.item())\n",
    "        except:\n",
    "            ys.append(y)\n",
    "    return xs, ys\n",
    "\n",
    "def get_normalized_data(dataset):\n",
    "    if dataset == 'MNIST':\n",
    "        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data/', train=True, download=True)\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
    "    else:\n",
    "        print('dataset parameter should be MNIST or CIFAR10')\n",
    "        return\n",
    "    \n",
    "    train_x, train_y = reshaper(trainset, dataset)\n",
    "    train_x_normalized, train_y_normalized = normalize_data(train_x, train_y, dataset)\n",
    "    test_x, test_y = reshaper(testset, dataset)\n",
    "    test_x_normalized, test_y_normalized = normalize_data(test_x, test_y, dataset)\n",
    "    \n",
    "    return train_x_normalized, train_y_normalized, test_x_normalized, test_y_normalized\n",
    "\n",
    "# MNIST example\n",
    "DATASET = 'MNIST'\n",
    "train_x, train_y, test_x, test_y = get_normalized_data(DATASET)\n",
    "with open('./to_reproduce/shuffled_idx.pickle', 'rb') as f:\n",
    "    shuffled_idxs = pickle.load(f)\n",
    "N_SAMPLES = len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_size(D_0=False, L_k=False, epsilon=False, convex=False, fast=False, alpha=False, bs=False):\n",
    "    # batch size in different cases\n",
    "    if fast and alpha:\n",
    "        batch_size = min(max(int(D_0*alpha / (epsilon)), 1), N_SAMPLES)\n",
    "    elif bs:\n",
    "        batch_size = bs\n",
    "    else:\n",
    "        if convex:\n",
    "            batch_size = min(max(int(D_0 / (L_k * epsilon)), 1), N_SAMPLES)\n",
    "        else:\n",
    "            batch_size = min(max(int(8 * D_0 / (epsilon**2)), 1), N_SAMPLES)\n",
    "    return batch_size\n",
    "\n",
    "def get_batch(idx, train_x, train_y):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for i in idx:\n",
    "        if DATASET == 'MNIST':\n",
    "            batch_x.append(train_x[i, :])\n",
    "        elif DATASET == 'CIFAR10':\n",
    "            batch_x.append(train_x[i, :, :, :].reshape(1, 3, 32, 32))\n",
    "        batch_y.append(train_y[i])\n",
    "    return torch.FloatTensor(np.vstack(tuple(batch_x))), torch.LongTensor(batch_y)\n",
    "\n",
    "def evaluate_function(algo, x, y, crit):\n",
    "    with torch.no_grad():\n",
    "        out = algo(torch.FloatTensor(x))\n",
    "        loss = crit(out, torch.LongTensor(y))\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(28*28, 10, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.50454044342041\n",
      "epoch 0 done\n",
      "1.5877803564071655 10 0.5\n",
      "epoch 1 done\n",
      "1.4368382692337036 5 0.25\n",
      "epoch 2 done\n",
      "1.2743124961853027 10 0.5\n",
      "epoch 3 done\n",
      "1.1793426275253296 10 0.5\n",
      "epoch 4 done\n",
      "1.1036754846572876 10 0.5\n",
      "epoch 5 done\n",
      "1.0946258306503296 10 0.5\n",
      "epoch 6 done\n",
      "1.0790493488311768 10 0.5\n",
      "epoch 7 done\n",
      "1.0499826669692993 10 0.5\n",
      "epoch 8 done\n",
      "1.0385204553604126 10 0.5\n",
      "epoch 9 done\n",
      "0.9992709159851074 10 0.5\n",
      "epoch 10 done\n",
      "0.9850336313247681 10 0.5\n",
      "epoch 11 done\n",
      "0.9884915947914124 10 0.5\n",
      "epoch 12 done\n",
      "0.9892107248306274 10 0.5\n",
      "epoch 13 done\n",
      "0.9364581108093262 10 0.5\n",
      "epoch 14 done\n",
      "0.9255140423774719 10 0.5\n",
      "epoch 15 done\n",
      "0.9423025846481323 10 0.5\n",
      "epoch 16 done\n",
      "0.9702706933021545 10 0.5\n",
      "epoch 17 done\n",
      "0.9591780304908752 10 0.5\n",
      "epoch 18 done\n",
      "0.9329747557640076 10 0.5\n",
      "epoch 19 done\n",
      "0.9309083223342896 10 0.5\n",
      "epoch 20 done\n",
      "0.9382079243659973 10 0.5\n",
      "epoch 21 done\n",
      "0.9188967943191528 10 0.5\n",
      "epoch 22 done\n",
      "0.9387637376785278 10 0.5\n",
      "epoch 23 done\n",
      "0.9386370182037354 10 0.5\n",
      "epoch 24 done\n",
      "0.9217556118965149 10 0.5\n",
      "epoch 25 done\n",
      "0.9151530861854553 10 0.5\n",
      "epoch 26 done\n",
      "0.9270302653312683 10 0.5\n",
      "epoch 27 done\n",
      "0.9275056719779968 10 0.5\n",
      "epoch 28 done\n",
      "0.9331790208816528 10 0.5\n",
      "epoch 29 done\n",
      "0.9277631044387817 10 0.5\n",
      "epoch 30 done\n",
      "0.9596045613288879 10 0.5\n",
      "epoch 31 done\n",
      "1.0117026567459106 10 0.5\n",
      "epoch 32 done\n",
      "1.0210202932357788 10 0.5\n",
      "epoch 33 done\n",
      "1.0371237993240356 10 0.5\n",
      "epoch 34 done\n",
      "1.0486699342727661 10 0.5\n",
      "epoch 35 done\n",
      "1.0768109560012817 10 0.5\n",
      "epoch 36 done\n",
      "1.0721455812454224 10 0.5\n",
      "epoch 37 done\n",
      "1.0713748931884766 10 0.5\n",
      "epoch 38 done\n",
      "1.0425866842269897 10 0.5\n",
      "epoch 39 done\n",
      "1.0340771675109863 10 0.5\n",
      "epoch 40 done\n",
      "1.0186891555786133 10 0.5\n",
      "epoch 41 done\n",
      "1.0167112350463867 10 0.5\n",
      "epoch 42 done\n",
      "0.9991124272346497 10 0.5\n",
      "epoch 43 done\n",
      "1.0185986757278442 10 0.5\n",
      "epoch 44 done\n",
      "1.0185037851333618 10 0.5\n",
      "epoch 45 done\n",
      "1.0274592638015747 10 0.5\n",
      "epoch 46 done\n",
      "1.0319716930389404 10 0.5\n",
      "epoch 47 done\n",
      "1.0422234535217285 10 0.5\n",
      "epoch 48 done\n",
      "1.071704626083374 10 0.5\n",
      "epoch 49 done\n",
      "1.0422111749649048 10 0.5\n",
      "epoch 50 done\n",
      "1.0126807689666748 10 0.5\n",
      "epoch 51 done\n",
      "1.0323312282562256 10 0.5\n",
      "epoch 52 done\n",
      "1.0429636240005493 10 0.5\n",
      "epoch 53 done\n",
      "1.0611560344696045 10 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-198471471f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mflatten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoosen_coordinate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0mloss_for_grad_estim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_to_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_to_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                     \u001b[0;31m#print(loss_before, loss_for_grad_estim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8e57a52d2c93>\u001b[0m in \u001b[0;36mevaluate_function\u001b[0;34m(algo, x, y, crit)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 942\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 1000\n",
    "N_SAMPLES = len(train_y)\n",
    "net = LogisticRegression()\n",
    "N_PARAMS = sum([x.numel() for x in net.parameters()])\n",
    "net.load_state_dict(torch.load('./to_reproduce/lr_starting_points/lr_starting_point_1'))\n",
    "convex = True # True is for Alg. 2 in paper, False is for Alg. 5 in paper\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "print(evaluate_function(net, train_x, train_y, criterion))\n",
    "D_0, epsilon, L_k, min_L_k, tau = (0.001, 0.0001, 10000., 1., 0.000001)\n",
    "\n",
    "iter_losses = []\n",
    "iter_train_accs = []\n",
    "iter_test_accs = []\n",
    "iter_times = []\n",
    "iter_batch_size = []\n",
    "iter_steps = []\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_train_accs = []\n",
    "epoch_test_accs = []\n",
    "epoch_times = []\n",
    "epoch_iterations = []\n",
    "\n",
    "epoch_done = False\n",
    "for epoch in range(n_epoch):\n",
    "    start_idx = 0\n",
    "    while True:\n",
    "        i_k = 0\n",
    "        #print(L_k)\n",
    "        while True:\n",
    "            new_L_k = (2**(i_k - 1))*L_k\n",
    "            if new_L_k < min_L_k:\n",
    "                L_k_1 = min_L_k\n",
    "            else:\n",
    "                L_k_1 = new_L_k\n",
    "\n",
    "            \n",
    "            bs_to_check = get_batch_size(D_0=D_0, L_k=L_k_1, epsilon=epsilon, convex=convex)\n",
    "\n",
    "            end_idx = start_idx + bs_to_check\n",
    "            if start_idx == 0 and bs_to_check >= N_SAMPLES:\n",
    "                epoch_done = True\n",
    "                x_to_check = torch.FloatTensor(train_x)\n",
    "                y_to_check = torch.LongTensor(train_y)\n",
    "            elif start_idx != 0 and end_idx > N_SAMPLES:\n",
    "                epoch_done = True\n",
    "                idx_1 = shuffled_idxs[start_idx: N_SAMPLES]\n",
    "                idx_2 = shuffled_idxs[: bs_to_check - (N_SAMPLES - start_idx)]\n",
    "                idx_to_check = idx_1 + idx_2\n",
    "                x_to_check, y_to_check = get_batch(idx_to_check, train_x, train_y)\n",
    "            elif end_idx <= N_SAMPLES:\n",
    "                idx_to_check = shuffled_idxs[start_idx: end_idx]\n",
    "                x_to_check, y_to_check = get_batch(idx_to_check, train_x, train_y)\n",
    "                \n",
    "            \n",
    "            loss_before = evaluate_function(net, x_to_check, y_to_check, criterion)\n",
    "            with torch.no_grad():\n",
    "                old_params = list(net.parameters())\n",
    "                \n",
    "                choosen_param = np.random.choice(range(len(old_params)))\n",
    "                #print(choosen_param)\n",
    "                choosen_coordinate = np.random.choice(range(old_params[choosen_param].numel()))\n",
    "                #print(choosen_coordinate)\n",
    "\n",
    "                for i, p in enumerate(list(net.parameters())):\n",
    "                    if i != choosen_param:\n",
    "                        continue\n",
    "                    old_p = p.data\n",
    "                    flatten = p.data.view(-1)\n",
    "                    flatten[choosen_coordinate] += tau\n",
    "                    p.data = flatten.view(p.size())\n",
    "                    loss_for_grad_estim = evaluate_function(net, x_to_check, y_to_check, criterion)\n",
    "                    #print(loss_before, loss_for_grad_estim)\n",
    "                    p.data = old_p\n",
    "                    \n",
    "                    new_flatten = p.data.view(-1)\n",
    "                    coordinate_grad = (1 / tau) * (loss_for_grad_estim - loss_before)\n",
    "                    new_flatten[choosen_coordinate] -= (1/(2*L_k_1))*coordinate_grad\n",
    "                    p.data = new_flatten.view(p.size())\n",
    "            loss_after = evaluate_function(net, x_to_check, y_to_check, criterion)\n",
    "            \n",
    "            if loss_after <= loss_before - (1/(4*L_k_1))*coordinate_grad**2 + epsilon/2:\n",
    "                #print(loss_after, loss_before)\n",
    "                #print()\n",
    "                #print(evaluate_function(net, train_x, train_y, criterion), 1/(2*L_k), bs_to_check)\n",
    "                start_idx = end_idx\n",
    "                L_k = L_k_1\n",
    "                if epoch_done:\n",
    "                    with torch.no_grad():\n",
    "                        out_train = net(torch.from_numpy(train_x))\n",
    "                        out_test = net(torch.from_numpy(test_x))\n",
    "                        iter_loss = criterion(out_train, torch.from_numpy(np.array(train_y))).item()\n",
    "\n",
    "                        pred_test = np.argmax(out_test.detach().numpy(),axis=1)\n",
    "                        ground_test = np.array(test_y)\n",
    "                        l = float(len(ground_test))\n",
    "                        iter_test_acc = len(np.where(pred_test == ground_test)[0]) / l\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                for i, p in enumerate(list(net.parameters())):\n",
    "                    if i != choosen_param:\n",
    "                        continue\n",
    "                    p.data = old_p\n",
    "                i_k += 1  \n",
    "        if epoch_done:\n",
    "            epoch_losses.append(iter_loss)\n",
    "            epoch_test_accs.append(iter_test_acc)\n",
    "            print('epoch {} done'.format(epoch))\n",
    "            print(iter_loss, bs_to_check, 1/(2*L_k))\n",
    "            epoch_done = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_to_check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
