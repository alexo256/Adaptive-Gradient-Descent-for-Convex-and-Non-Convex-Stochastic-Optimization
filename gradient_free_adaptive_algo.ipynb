{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x, y, dataset):\n",
    "    pairs = list(zip(x, y))\n",
    "    np.random.shuffle(pairs)\n",
    "    x_shuffled = tuple([x[0] for x in pairs])\n",
    "    y_shuffled = tuple([x[1] for x in pairs])\n",
    "    \n",
    "    if dataset == 'MNIST':\n",
    "        x_shuffled = np.array(x_shuffled).reshape(-1, 784)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        x_shuffled = np.array(x_shuffled).reshape(-1, 3, 32, 32)\n",
    "    else:\n",
    "        print('dataset parameter should be MNIST or CIFAR10')\n",
    "        return\n",
    "    mean_x = np.mean(x_shuffled, axis=0).reshape(1, -1)\n",
    "    std_x = np.std(x_shuffled)\n",
    "\n",
    "    x_shuffled = (x_shuffled - mean_x) / std_x\n",
    "    return x_shuffled, y_shuffled\n",
    "\n",
    "def reshaper(torch_dataset, dataset):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for x, y in torch_dataset:\n",
    "        if dataset == 'MNIST':\n",
    "            xs.append(np.array(x, dtype=np.float32).reshape(1, -1))\n",
    "        else:\n",
    "            xs.append(np.array(x, dtype=np.float32).reshape(3, 32, 32))\n",
    "        try:\n",
    "            ys.append(y.item())\n",
    "        except:\n",
    "            ys.append(y)\n",
    "    return xs, ys\n",
    "\n",
    "def get_normalized_data(dataset):\n",
    "    if dataset == 'MNIST':\n",
    "        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data/', train=True, download=True)\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
    "    else:\n",
    "        print('dataset parameter should be MNIST or CIFAR10')\n",
    "        return\n",
    "    \n",
    "    train_x, train_y = reshaper(trainset, dataset)\n",
    "    train_x_normalized, train_y_normalized = normalize_data(train_x, train_y, dataset)\n",
    "    test_x, test_y = reshaper(testset, dataset)\n",
    "    test_x_normalized, test_y_normalized = normalize_data(test_x, test_y, dataset)\n",
    "    \n",
    "    return train_x_normalized, train_y_normalized, test_x_normalized, test_y_normalized\n",
    "\n",
    "# MNIST example\n",
    "DATASET = 'MNIST'\n",
    "train_x, train_y, test_x, test_y = get_normalized_data(DATASET)\n",
    "with open('./to_reproduce/shuffled_idx.pickle', 'rb') as f:\n",
    "    shuffled_idxs = pickle.load(f)\n",
    "N_SAMPLES = len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_size(D_0=False, L_k=False, epsilon=False, convex=False, fast=False, alpha=False, bs=False):\n",
    "    # batch size in different cases\n",
    "    if fast and alpha:\n",
    "        batch_size = min(max(int(D_0*alpha / (epsilon)), 1), N_SAMPLES)\n",
    "    elif bs:\n",
    "        batch_size = bs\n",
    "    else:\n",
    "        if convex:\n",
    "            batch_size = min(max(int(D_0 / (L_k * epsilon)), 1), N_SAMPLES)\n",
    "        else:\n",
    "            batch_size = min(max(int(8 * D_0 / (epsilon**2)), 1), N_SAMPLES)\n",
    "    return batch_size\n",
    "\n",
    "def get_batch(idx, train_x, train_y):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for i in idx:\n",
    "        if DATASET == 'MNIST':\n",
    "            batch_x.append(train_x[i, :])\n",
    "        elif DATASET == 'CIFAR10':\n",
    "            batch_x.append(train_x[i, :, :, :].reshape(1, 3, 32, 32))\n",
    "        batch_y.append(train_y[i])\n",
    "    return torch.FloatTensor(np.vstack(tuple(batch_x))), torch.LongTensor(batch_y)\n",
    "\n",
    "def evaluate_function(algo, x, y, crit, reduction='mean'):\n",
    "    crit = crit(reduction=reduction)\n",
    "    with torch.no_grad():\n",
    "        out = algo(torch.FloatTensor(x))\n",
    "        if reduction != 'none':\n",
    "            loss = crit(out, torch.LongTensor(y))\n",
    "        else:\n",
    "            return crit(out, torch.LongTensor(y))\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(28*28, 10, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm with first choice of L_k and r_k after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.50454044342041\n",
      "epoch 0 done\n",
      "1.57681405544281 10 0.5\n",
      "epoch 1 done\n",
      "1.1900826692581177 10 0.5\n",
      "epoch 2 done\n",
      "1.0925655364990234 10 0.5\n",
      "epoch 3 done\n",
      "1.047715663909912 10 0.5\n",
      "epoch 4 done\n",
      "1.0616923570632935 10 0.5\n",
      "epoch 5 done\n",
      "0.9943814873695374 10 0.5\n",
      "epoch 6 done\n",
      "0.9368994832038879 10 0.5\n",
      "epoch 7 done\n",
      "0.9057785272598267 10 0.5\n",
      "epoch 8 done\n",
      "0.8992491364479065 10 0.5\n",
      "epoch 9 done\n",
      "0.8852349519729614 10 0.5\n",
      "epoch 10 done\n",
      "0.8666549324989319 10 0.5\n",
      "epoch 11 done\n",
      "0.8907153606414795 10 0.5\n",
      "epoch 12 done\n",
      "0.8911431431770325 5 0.25\n",
      "epoch 13 done\n",
      "0.8765305876731873 10 0.5\n",
      "epoch 14 done\n",
      "0.8518108129501343 10 0.5\n",
      "epoch 15 done\n",
      "0.8617379069328308 10 0.5\n",
      "epoch 16 done\n",
      "0.8875938653945923 10 0.5\n",
      "epoch 17 done\n",
      "0.9044677019119263 10 0.5\n",
      "epoch 18 done\n",
      "0.9305834174156189 10 0.5\n",
      "epoch 19 done\n",
      "0.9438886046409607 10 0.5\n",
      "epoch 20 done\n",
      "0.9495401978492737 10 0.5\n",
      "epoch 21 done\n",
      "0.9689264893531799 10 0.5\n",
      "epoch 22 done\n",
      "0.9618505835533142 10 0.5\n",
      "epoch 23 done\n",
      "0.9983253479003906 10 0.5\n",
      "epoch 24 done\n",
      "0.9981207847595215 10 0.5\n",
      "epoch 25 done\n",
      "1.0264029502868652 10 0.5\n",
      "epoch 26 done\n",
      "0.9822260141372681 10 0.5\n",
      "epoch 27 done\n",
      "1.0037978887557983 10 0.5\n",
      "epoch 28 done\n",
      "0.9861029386520386 10 0.5\n",
      "epoch 29 done\n",
      "0.9721719026565552 10 0.5\n",
      "epoch 30 done\n",
      "0.9806880950927734 10 0.5\n",
      "epoch 31 done\n",
      "0.9908384084701538 10 0.5\n",
      "epoch 32 done\n",
      "0.9782360196113586 10 0.5\n",
      "epoch 33 done\n",
      "0.9717668890953064 10 0.5\n",
      "epoch 34 done\n",
      "0.971723198890686 10 0.5\n",
      "epoch 35 done\n",
      "0.971605658531189 10 0.5\n",
      "epoch 36 done\n",
      "0.9588757157325745 10 0.5\n",
      "epoch 37 done\n",
      "0.9559702277183533 10 0.5\n",
      "epoch 38 done\n",
      "0.9660859107971191 10 0.5\n",
      "epoch 39 done\n",
      "0.9754366278648376 10 0.5\n",
      "epoch 40 done\n",
      "0.9664934277534485 10 0.5\n",
      "epoch 41 done\n",
      "0.9608376026153564 10 0.5\n",
      "epoch 42 done\n",
      "0.9707509875297546 10 0.5\n",
      "epoch 43 done\n",
      "0.9515637159347534 10 0.5\n",
      "epoch 44 done\n",
      "0.934868335723877 10 0.5\n",
      "epoch 45 done\n",
      "0.9582382440567017 10 0.5\n",
      "epoch 46 done\n",
      "0.9477437734603882 10 0.5\n",
      "epoch 47 done\n",
      "0.9408618211746216 10 0.5\n",
      "epoch 48 done\n",
      "0.964102029800415 10 0.5\n",
      "epoch 49 done\n",
      "0.9758344888687134 10 0.5\n",
      "epoch 50 done\n",
      "0.9786565899848938 10 0.5\n",
      "epoch 51 done\n",
      "0.9634783864021301 10 0.5\n",
      "epoch 52 done\n",
      "0.9808114767074585 10 0.5\n",
      "epoch 53 done\n",
      "0.97512286901474 10 0.5\n",
      "epoch 54 done\n",
      "0.982430100440979 10 0.5\n",
      "epoch 55 done\n",
      "0.9699193239212036 10 0.5\n",
      "epoch 56 done\n",
      "0.9748712778091431 10 0.5\n",
      "epoch 57 done\n",
      "0.9749254584312439 10 0.5\n",
      "epoch 58 done\n",
      "0.984565019607544 10 0.5\n",
      "epoch 59 done\n",
      "0.9852325916290283 10 0.5\n",
      "epoch 60 done\n",
      "0.9921907782554626 10 0.5\n",
      "epoch 61 done\n",
      "0.9875812530517578 10 0.5\n",
      "epoch 62 done\n",
      "1.014883279800415 10 0.5\n",
      "epoch 63 done\n",
      "1.0062540769577026 10 0.5\n",
      "epoch 64 done\n",
      "1.0154809951782227 10 0.5\n",
      "epoch 65 done\n",
      "1.0295501947402954 10 0.5\n",
      "epoch 66 done\n",
      "1.0281866788864136 10 0.5\n",
      "epoch 67 done\n",
      "1.0346810817718506 10 0.5\n",
      "epoch 68 done\n",
      "1.053830623626709 10 0.5\n",
      "epoch 69 done\n",
      "1.0483413934707642 10 0.5\n",
      "epoch 70 done\n",
      "1.0498751401901245 10 0.5\n",
      "epoch 71 done\n",
      "1.037660837173462 10 0.5\n",
      "epoch 72 done\n",
      "1.064447045326233 10 0.5\n",
      "epoch 73 done\n",
      "1.0637480020523071 10 0.5\n",
      "epoch 74 done\n",
      "1.0717874765396118 10 0.5\n",
      "epoch 75 done\n",
      "1.0813591480255127 10 0.5\n",
      "epoch 76 done\n",
      "1.0822354555130005 10 0.5\n",
      "epoch 77 done\n",
      "1.0780503749847412 10 0.5\n",
      "epoch 78 done\n",
      "1.094573974609375 10 0.5\n",
      "epoch 79 done\n",
      "1.113368272781372 10 0.5\n",
      "epoch 80 done\n",
      "1.1091805696487427 10 0.5\n",
      "epoch 81 done\n",
      "1.1058800220489502 10 0.5\n",
      "epoch 82 done\n",
      "1.1402156352996826 10 0.5\n",
      "epoch 83 done\n",
      "1.1435153484344482 10 0.5\n",
      "epoch 84 done\n",
      "1.1397230625152588 10 0.5\n",
      "epoch 85 done\n",
      "1.1240837574005127 10 0.5\n",
      "epoch 86 done\n",
      "1.1324412822723389 10 0.5\n",
      "epoch 87 done\n",
      "1.1286486387252808 10 0.5\n",
      "epoch 88 done\n",
      "1.1261008977890015 10 0.5\n",
      "epoch 89 done\n",
      "1.1282470226287842 10 0.5\n",
      "epoch 90 done\n",
      "1.1571177244186401 10 0.5\n",
      "epoch 91 done\n",
      "1.1398825645446777 10 0.5\n",
      "epoch 92 done\n",
      "1.11258065700531 10 0.5\n",
      "epoch 93 done\n",
      "1.109633207321167 10 0.5\n",
      "epoch 94 done\n",
      "1.0980044603347778 10 0.5\n",
      "epoch 95 done\n",
      "1.0843592882156372 10 0.5\n",
      "epoch 96 done\n",
      "1.10102117061615 10 0.5\n",
      "epoch 97 done\n",
      "1.100257396697998 10 0.5\n",
      "epoch 98 done\n",
      "1.1254119873046875 10 0.5\n",
      "epoch 99 done\n",
      "1.1158095598220825 10 0.5\n",
      "epoch 100 done\n",
      "1.1412421464920044 10 0.5\n",
      "epoch 101 done\n",
      "1.1346803903579712 10 0.5\n",
      "epoch 102 done\n",
      "1.174555778503418 10 0.5\n",
      "epoch 103 done\n",
      "1.1336709260940552 10 0.5\n",
      "epoch 104 done\n",
      "1.1222965717315674 10 0.5\n",
      "epoch 105 done\n",
      "1.111460566520691 10 0.5\n",
      "epoch 106 done\n",
      "1.128431797027588 10 0.5\n",
      "epoch 107 done\n",
      "1.1366591453552246 10 0.5\n",
      "epoch 108 done\n",
      "1.1166490316390991 10 0.5\n",
      "epoch 109 done\n",
      "1.1149139404296875 10 0.5\n",
      "epoch 110 done\n",
      "1.132215142250061 10 0.5\n",
      "epoch 111 done\n",
      "1.1505569219589233 10 0.5\n",
      "epoch 112 done\n",
      "1.165201187133789 10 0.5\n",
      "epoch 113 done\n",
      "1.1766761541366577 10 0.5\n",
      "epoch 114 done\n",
      "1.2009892463684082 10 0.5\n",
      "epoch 115 done\n",
      "1.2144885063171387 10 0.5\n",
      "epoch 116 done\n",
      "1.2294679880142212 10 0.5\n",
      "epoch 117 done\n",
      "1.1941949129104614 10 0.5\n",
      "epoch 118 done\n",
      "1.1955326795578003 10 0.5\n",
      "epoch 119 done\n",
      "1.1781411170959473 10 0.5\n",
      "epoch 120 done\n",
      "1.169087529182434 10 0.5\n",
      "epoch 121 done\n",
      "1.1768587827682495 10 0.5\n",
      "epoch 122 done\n",
      "1.1825264692306519 10 0.5\n",
      "epoch 123 done\n",
      "1.1932874917984009 10 0.5\n",
      "epoch 124 done\n",
      "1.2119165658950806 10 0.5\n",
      "epoch 125 done\n",
      "1.2093381881713867 10 0.5\n",
      "epoch 126 done\n",
      "1.211815357208252 10 0.5\n",
      "epoch 127 done\n",
      "1.2094943523406982 10 0.5\n",
      "epoch 128 done\n",
      "1.2159699201583862 10 0.5\n",
      "epoch 129 done\n",
      "1.218384861946106 10 0.5\n",
      "epoch 130 done\n",
      "1.1967504024505615 10 0.5\n",
      "epoch 131 done\n",
      "1.2190207242965698 10 0.5\n",
      "epoch 132 done\n",
      "1.2221792936325073 10 0.5\n",
      "epoch 133 done\n",
      "1.2662841081619263 10 0.5\n",
      "epoch 134 done\n",
      "1.2706490755081177 10 0.5\n",
      "epoch 135 done\n",
      "1.2502193450927734 10 0.5\n",
      "epoch 136 done\n",
      "1.2752864360809326 10 0.5\n",
      "epoch 137 done\n",
      "1.2567659616470337 10 0.5\n",
      "epoch 138 done\n",
      "1.2493503093719482 10 0.5\n",
      "epoch 139 done\n",
      "1.2553092241287231 10 0.5\n",
      "epoch 140 done\n",
      "1.2682052850723267 10 0.5\n",
      "epoch 141 done\n",
      "1.279771089553833 10 0.5\n",
      "epoch 142 done\n",
      "1.284896969795227 10 0.5\n",
      "epoch 143 done\n",
      "1.2709109783172607 10 0.5\n",
      "epoch 144 done\n",
      "1.287140965461731 10 0.5\n",
      "epoch 145 done\n",
      "1.3024572134017944 10 0.5\n",
      "epoch 146 done\n",
      "1.3038206100463867 10 0.5\n",
      "epoch 147 done\n",
      "1.320322871208191 10 0.5\n",
      "epoch 148 done\n",
      "1.343225359916687 10 0.5\n",
      "epoch 149 done\n",
      "1.3534862995147705 10 0.5\n",
      "epoch 150 done\n",
      "1.3389705419540405 10 0.5\n",
      "epoch 151 done\n",
      "1.3703898191452026 10 0.5\n",
      "epoch 152 done\n",
      "1.3407312631607056 10 0.5\n",
      "epoch 153 done\n",
      "1.3623645305633545 10 0.5\n",
      "epoch 154 done\n",
      "1.349993109703064 10 0.5\n",
      "epoch 155 done\n",
      "1.3693267107009888 10 0.5\n",
      "epoch 156 done\n",
      "1.373193383216858 10 0.5\n",
      "epoch 157 done\n",
      "1.368422269821167 10 0.5\n",
      "epoch 158 done\n",
      "1.3779411315917969 10 0.5\n",
      "epoch 159 done\n",
      "1.3401902914047241 10 0.5\n",
      "epoch 160 done\n",
      "1.3398997783660889 10 0.5\n",
      "epoch 161 done\n",
      "1.3368492126464844 10 0.5\n",
      "epoch 162 done\n",
      "1.350909948348999 10 0.5\n",
      "epoch 163 done\n",
      "1.3546979427337646 10 0.5\n",
      "epoch 164 done\n",
      "1.3531156778335571 10 0.5\n",
      "epoch 165 done\n",
      "1.3470309972763062 10 0.5\n",
      "epoch 166 done\n",
      "1.316641926765442 10 0.5\n",
      "epoch 167 done\n",
      "1.3400213718414307 10 0.5\n",
      "epoch 168 done\n",
      "1.3351969718933105 10 0.5\n",
      "epoch 169 done\n",
      "1.347853660583496 10 0.5\n",
      "epoch 170 done\n",
      "1.3255609273910522 10 0.5\n",
      "epoch 171 done\n",
      "1.3325915336608887 10 0.5\n",
      "epoch 172 done\n",
      "1.314787745475769 10 0.5\n",
      "epoch 173 done\n",
      "1.3108476400375366 10 0.5\n",
      "epoch 174 done\n",
      "1.3340836763381958 10 0.5\n",
      "epoch 175 done\n",
      "1.3298133611679077 10 0.5\n",
      "epoch 176 done\n",
      "1.3262466192245483 10 0.5\n",
      "epoch 177 done\n",
      "1.354400873184204 10 0.5\n",
      "epoch 178 done\n",
      "1.3672006130218506 5 0.25\n",
      "epoch 179 done\n",
      "1.3631904125213623 10 0.5\n",
      "epoch 180 done\n",
      "1.3671245574951172 10 0.5\n",
      "epoch 181 done\n",
      "1.3606271743774414 10 0.5\n",
      "epoch 182 done\n",
      "1.3731437921524048 10 0.5\n",
      "epoch 183 done\n",
      "1.392299771308899 10 0.5\n",
      "epoch 184 done\n",
      "1.4106214046478271 10 0.5\n",
      "epoch 185 done\n",
      "1.4091283082962036 10 0.5\n",
      "epoch 186 done\n",
      "1.4342252016067505 10 0.5\n",
      "epoch 187 done\n",
      "1.4465609788894653 10 0.5\n",
      "epoch 188 done\n",
      "1.4632346630096436 10 0.5\n",
      "epoch 189 done\n",
      "1.4613735675811768 10 0.5\n",
      "epoch 190 done\n",
      "1.4537272453308105 10 0.5\n",
      "epoch 191 done\n",
      "1.4460841417312622 10 0.5\n",
      "epoch 192 done\n",
      "1.4414297342300415 10 0.5\n",
      "epoch 193 done\n",
      "1.4692130088806152 10 0.5\n",
      "epoch 194 done\n",
      "1.48586106300354 10 0.5\n",
      "epoch 195 done\n",
      "1.4797449111938477 10 0.5\n",
      "epoch 196 done\n",
      "1.4621961116790771 10 0.5\n",
      "epoch 197 done\n",
      "1.463396668434143 10 0.5\n",
      "epoch 198 done\n",
      "1.45490300655365 10 0.5\n",
      "epoch 199 done\n",
      "1.4443998336791992 10 0.5\n",
      "epoch 200 done\n",
      "1.4408457279205322 10 0.5\n",
      "epoch 201 done\n",
      "1.4488582611083984 10 0.5\n",
      "epoch 202 done\n",
      "1.4813069105148315 10 0.5\n",
      "epoch 203 done\n",
      "1.48068368434906 10 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 204 done\n",
      "1.4873372316360474 10 0.5\n",
      "epoch 205 done\n",
      "1.5027165412902832 10 0.5\n",
      "epoch 206 done\n",
      "1.4770244359970093 10 0.5\n",
      "epoch 207 done\n",
      "1.471001386642456 10 0.5\n",
      "epoch 208 done\n",
      "1.480560064315796 10 0.5\n",
      "epoch 209 done\n",
      "1.4741239547729492 10 0.5\n",
      "epoch 210 done\n",
      "1.4893995523452759 10 0.5\n",
      "epoch 211 done\n",
      "1.4685766696929932 10 0.5\n",
      "epoch 212 done\n",
      "1.459059238433838 10 0.5\n",
      "epoch 213 done\n",
      "1.468521237373352 10 0.5\n",
      "epoch 214 done\n",
      "1.459505319595337 10 0.5\n",
      "epoch 215 done\n",
      "1.4624929428100586 10 0.5\n",
      "epoch 216 done\n",
      "1.4498425722122192 10 0.5\n",
      "epoch 217 done\n",
      "1.4474676847457886 10 0.5\n",
      "epoch 218 done\n",
      "1.4548194408416748 10 0.5\n",
      "epoch 219 done\n",
      "1.4648936986923218 10 0.5\n",
      "epoch 220 done\n",
      "1.4772231578826904 10 0.5\n",
      "epoch 221 done\n",
      "1.4794502258300781 10 0.5\n",
      "epoch 222 done\n",
      "1.4705603122711182 10 0.5\n",
      "epoch 223 done\n",
      "1.4636856317520142 10 0.5\n",
      "epoch 224 done\n",
      "1.4425731897354126 10 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f21266fc64c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mchoosen_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m#print(choosen_param)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mchoosen_coordinate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoosen_param\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0;31m#print(choosen_coordinate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 1000\n",
    "N_SAMPLES = len(train_y)\n",
    "net = LogisticRegression()\n",
    "N_PARAMS = sum([x.numel() for x in net.parameters()])\n",
    "net.load_state_dict(torch.load('./to_reproduce/lr_starting_points/lr_starting_point_1'))\n",
    "convex = True # True is for Alg. 2 in paper, False is for Alg. 5 in paper\n",
    "criterion = nn.CrossEntropyLoss\n",
    "criterion_ = nn.CrossEntropyLoss(reduction='mean')\n",
    "print(evaluate_function(net, train_x, train_y, criterion))\n",
    "D_0, epsilon, L_k, min_L_k, tau = (0.001, 0.0001, 10000., 1., 0.000001)\n",
    "\n",
    "iter_losses = []\n",
    "iter_train_accs = []\n",
    "iter_test_accs = []\n",
    "iter_times = []\n",
    "iter_batch_size = []\n",
    "iter_steps = []\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_train_accs = []\n",
    "epoch_test_accs = []\n",
    "epoch_times = []\n",
    "epoch_iterations = []\n",
    "\n",
    "epoch_done = False\n",
    "for epoch in range(n_epoch):\n",
    "    start_idx = 0\n",
    "    while True:\n",
    "        i_k = 0\n",
    "        #print(L_k)\n",
    "        while True:\n",
    "            new_L_k = (2**(i_k - 1))*L_k\n",
    "            if new_L_k < min_L_k:\n",
    "                L_k_1 = min_L_k\n",
    "            else:\n",
    "                L_k_1 = new_L_k\n",
    "\n",
    "            \n",
    "            bs_to_check = get_batch_size(D_0=D_0, L_k=L_k_1, epsilon=epsilon, convex=convex)\n",
    "\n",
    "            end_idx = start_idx + bs_to_check\n",
    "            if start_idx == 0 and bs_to_check >= N_SAMPLES:\n",
    "                epoch_done = True\n",
    "                x_to_check = torch.FloatTensor(train_x)\n",
    "                y_to_check = torch.LongTensor(train_y)\n",
    "            elif start_idx != 0 and end_idx > N_SAMPLES:\n",
    "                epoch_done = True\n",
    "                idx_1 = shuffled_idxs[start_idx: N_SAMPLES]\n",
    "                idx_2 = shuffled_idxs[: bs_to_check - (N_SAMPLES - start_idx)]\n",
    "                idx_to_check = idx_1 + idx_2\n",
    "                x_to_check, y_to_check = get_batch(idx_to_check, train_x, train_y)\n",
    "            elif end_idx <= N_SAMPLES:\n",
    "                idx_to_check = shuffled_idxs[start_idx: end_idx]\n",
    "                x_to_check, y_to_check = get_batch(idx_to_check, train_x, train_y)\n",
    "                \n",
    "            \n",
    "            loss_before = evaluate_function(net, x_to_check, y_to_check, criterion)\n",
    "            with torch.no_grad():\n",
    "                old_params = list(net.parameters())\n",
    "                \n",
    "                choosen_param = np.random.choice(range(len(old_params)))\n",
    "                #print(choosen_param)\n",
    "                choosen_coordinate = np.random.choice(range(old_params[choosen_param].numel()))\n",
    "                #print(choosen_coordinate)\n",
    "\n",
    "                for i, p in enumerate(list(net.parameters())):\n",
    "                    if i != choosen_param:\n",
    "                        continue\n",
    "                    old_p = p.data\n",
    "                    flatten = p.data.view(-1)\n",
    "                    flatten[choosen_coordinate] += tau\n",
    "                    p.data = flatten.view(p.size())\n",
    "                    loss_for_grad_estim = evaluate_function(net, x_to_check, y_to_check, criterion)\n",
    "                    #print(loss_before, loss_for_grad_estim)\n",
    "                    p.data = old_p\n",
    "                    \n",
    "                    new_flatten = p.data.view(-1)\n",
    "                    coordinate_grad = (1 / tau) * (loss_for_grad_estim - loss_before)\n",
    "                    new_flatten[choosen_coordinate] -= (1/(2*L_k_1))*coordinate_grad\n",
    "                    p.data = new_flatten.view(p.size())\n",
    "            loss_after = evaluate_function(net, x_to_check, y_to_check, criterion)\n",
    "            \n",
    "            if loss_after <= loss_before - (1/(4*L_k_1))*coordinate_grad**2 + epsilon/2:\n",
    "                #print(loss_after, loss_before)\n",
    "                #print()\n",
    "                #print(evaluate_function(net, train_x, train_y, criterion), 1/(2*L_k), bs_to_check)\n",
    "                start_idx = end_idx\n",
    "                L_k = L_k_1\n",
    "                if epoch_done:\n",
    "                    with torch.no_grad():\n",
    "                        out_train = net(torch.from_numpy(train_x))\n",
    "                        out_test = net(torch.from_numpy(test_x))\n",
    "                        iter_loss = criterion_(out_train, torch.from_numpy(np.array(train_y))).item()\n",
    "\n",
    "                        pred_test = np.argmax(out_test.detach().numpy(),axis=1)\n",
    "                        ground_test = np.array(test_y)\n",
    "                        l = float(len(ground_test))\n",
    "                        iter_test_acc = len(np.where(pred_test == ground_test)[0]) / l\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                for i, p in enumerate(list(net.parameters())):\n",
    "                    if i != choosen_param:\n",
    "                        continue\n",
    "                    p.data = old_p\n",
    "                i_k += 1  \n",
    "        if epoch_done:\n",
    "            epoch_losses.append(iter_loss)\n",
    "            epoch_test_accs.append(iter_test_acc)\n",
    "            print('epoch {} done'.format(epoch))\n",
    "            print(iter_loss, bs_to_check, 1/(2*L_k))\n",
    "            epoch_done = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm with first choice of r_k and L_k after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5045392513275146\n",
      "epoch 0 done\n",
      "2.380889415740967 2 0.0035184372232947188\n",
      "epoch 1 done\n",
      "2.316734790802002 2 0.0035184372232947188\n",
      "epoch 2 done\n",
      "2.2482550144195557 2 0.1\n",
      "epoch 3 done\n",
      "2.1223816871643066 2 9.773436517870239e-05\n",
      "epoch 4 done\n",
      "2.0320193767547607 2 0.1\n",
      "epoch 5 done\n",
      "1.9769314527511597 2 0.1\n",
      "epoch 6 done\n",
      "1.8672014474868774 2 0.00021990232645591992\n",
      "epoch 7 done\n",
      "1.8124260902404785 2 0.1\n",
      "epoch 8 done\n",
      "1.7780168056488037 2 0.1\n",
      "epoch 9 done\n",
      "1.7117562294006348 2 0.1\n",
      "epoch 10 done\n",
      "1.676803469657898 2 0.1\n",
      "epoch 11 done\n",
      "1.7015706300735474 2 0.00021990232645591992\n",
      "epoch 12 done\n",
      "1.6983686685562134 2 0.1\n",
      "epoch 13 done\n",
      "1.6741758584976196 2 0.1\n",
      "epoch 14 done\n",
      "1.6275966167449951 2 0.1\n",
      "epoch 15 done\n",
      "1.5963523387908936 2 0.1\n",
      "epoch 16 done\n",
      "1.5417555570602417 2 0.1\n",
      "epoch 17 done\n",
      "1.5662568807601929 2 0.1\n",
      "epoch 18 done\n",
      "1.52008056640625 2 0.1\n",
      "epoch 19 done\n",
      "1.5124461650848389 2 0.1\n",
      "epoch 20 done\n",
      "1.487147569656372 2 0.1\n",
      "epoch 21 done\n",
      "1.4297336339950562 2 0.1\n",
      "epoch 22 done\n",
      "1.439789056777954 2 0.00021990232645591992\n",
      "epoch 23 done\n",
      "1.4008785486221313 2 0.1\n",
      "epoch 24 done\n",
      "1.3854260444641113 2 0.00021990232645591992\n",
      "epoch 25 done\n",
      "1.3812196254730225 2 0.1\n",
      "epoch 26 done\n",
      "1.3659793138504028 2 0.1\n",
      "epoch 27 done\n",
      "1.3600432872772217 2 0.1\n",
      "epoch 28 done\n",
      "1.3549132347106934 2 0.1\n",
      "epoch 29 done\n",
      "1.3579784631729126 2 0.1\n",
      "epoch 30 done\n",
      "1.3362617492675781 2 0.1\n",
      "epoch 31 done\n",
      "1.304703950881958 2 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b11cc96d2a34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mold_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mchoosen_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;31m#print(choosen_param)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mchoosen_coordinate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoosen_param\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 1000\n",
    "N_SAMPLES = len(train_y)\n",
    "net = LogisticRegression()\n",
    "N_PARAMS = sum([x.numel() for x in net.parameters()])\n",
    "net.load_state_dict(torch.load('./to_reproduce/lr_starting_points/lr_starting_point_1'))\n",
    "convex = True # True is for Alg. 2 in paper, False is for Alg. 5 in paper\n",
    "criterion = nn.CrossEntropyLoss\n",
    "criterion_ = nn.CrossEntropyLoss(reduction='mean')\n",
    "print(evaluate_function(net, train_x, train_y, criterion))\n",
    "D_0, epsilon, r_k, min_r_k, tau = (0.001, 0.0001, 100., 2., 0.000001)\n",
    "\n",
    "iter_losses = []\n",
    "iter_train_accs = []\n",
    "iter_test_accs = []\n",
    "iter_times = []\n",
    "iter_batch_size = []\n",
    "iter_steps = []\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_train_accs = []\n",
    "epoch_test_accs = []\n",
    "epoch_times = []\n",
    "epoch_iterations = []\n",
    "\n",
    "epoch_done = False\n",
    "for epoch in range(n_epoch):\n",
    "    start_idx = 0\n",
    "    while True:\n",
    "        i_k = 0\n",
    "        while True:\n",
    "            \n",
    "            new_r_k = (2**(i_k - 1))*r_k\n",
    "            if new_r_k < min_r_k:\n",
    "                r_k_1 = min_r_k\n",
    "            else:\n",
    "                r_k_1 = new_r_k\n",
    "\n",
    "            \n",
    "            bs_to_check = int(r_k_1)\n",
    "\n",
    "            end_idx = start_idx + bs_to_check\n",
    "            if start_idx == 0 and bs_to_check >= N_SAMPLES:\n",
    "                epoch_done = True\n",
    "                x_to_check = torch.FloatTensor(train_x)\n",
    "                y_to_check = torch.LongTensor(train_y)\n",
    "            elif start_idx != 0 and end_idx > N_SAMPLES:\n",
    "                epoch_done = True\n",
    "                idx_1 = shuffled_idxs[start_idx: N_SAMPLES]\n",
    "                idx_2 = shuffled_idxs[: bs_to_check - (N_SAMPLES - start_idx)]\n",
    "                idx_to_check = idx_1 + idx_2\n",
    "                x_to_check, y_to_check = get_batch(idx_to_check, train_x, train_y)\n",
    "            elif end_idx <= N_SAMPLES:\n",
    "                idx_to_check = shuffled_idxs[start_idx: end_idx]\n",
    "                x_to_check, y_to_check = get_batch(idx_to_check, train_x, train_y)\n",
    "                \n",
    "            \n",
    "            loss_before = evaluate_function(net, x_to_check, y_to_check, criterion, reduction='none')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                old_params = list(net.parameters())\n",
    "                \n",
    "                choosen_param = np.random.choice(range(len(old_params)))\n",
    "                #print(choosen_param)\n",
    "                choosen_coordinate = np.random.choice(range(old_params[choosen_param].numel()))\n",
    "                #print(choosen_coordinate)\n",
    "\n",
    "                for i, p in enumerate(list(net.parameters())):\n",
    "                    if i != choosen_param:\n",
    "                        continue\n",
    "                    old_p = p.data\n",
    "                    flatten = p.data.view(-1)\n",
    "                    flatten[choosen_coordinate] += tau\n",
    "                    p.data = flatten.view(p.size())\n",
    "                    loss_for_grad_estim = evaluate_function(net, x_to_check, y_to_check, criterion, reduction='none')\n",
    "                    \n",
    "                    p.data = old_p\n",
    "                    \n",
    "                    new_flatten = p.data.view(-1)\n",
    "                    variance_estim = (1 / tau) * (loss_for_grad_estim - loss_before)\n",
    "                    D_k_1 = (1/(r_k_1 - 1))*torch.sum((variance_estim - torch.mean(variance_estim))**2).item()\n",
    "                    if D_k_1 < D_0:\n",
    "                        D_k_1 = D_0\n",
    "                    L_k_1 = D_k_1/(r_k_1*epsilon)\n",
    "                    coordinate_grad = torch.mean(variance_estim)\n",
    "                    new_flatten[choosen_coordinate] -= (1/(2*L_k_1))*coordinate_grad\n",
    "                    p.data = new_flatten.view(p.size())\n",
    "            loss_after = evaluate_function(net, x_to_check, y_to_check, criterion)\n",
    "            \n",
    "            if loss_after <= torch.mean(loss_before) - (1/(4*L_k_1))*coordinate_grad**2 + epsilon/2:\n",
    "                #print(loss_after, loss_before)\n",
    "                #print()\n",
    "                #print(evaluate_function(net, train_x, train_y, criterion), 1/(2*L_k), bs_to_check)\n",
    "                start_idx = end_idx\n",
    "                r_k = r_k_1\n",
    "                if epoch_done:\n",
    "                    with torch.no_grad():\n",
    "                        out_train = net(torch.from_numpy(train_x))\n",
    "                        out_test = net(torch.from_numpy(test_x))\n",
    "                        iter_loss = criterion_(out_train, torch.from_numpy(np.array(train_y))).item()\n",
    "\n",
    "                        pred_test = np.argmax(out_test.detach().numpy(),axis=1)\n",
    "                        ground_test = np.array(test_y)\n",
    "                        l = float(len(ground_test))\n",
    "                        iter_test_acc = len(np.where(pred_test == ground_test)[0]) / l\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                for i, p in enumerate(list(net.parameters())):\n",
    "                    if i != choosen_param:\n",
    "                        continue\n",
    "                    p.data = old_p\n",
    "                i_k += 1  \n",
    "        if epoch_done:\n",
    "            epoch_losses.append(iter_loss)\n",
    "            epoch_test_accs.append(iter_test_acc)\n",
    "            print('epoch {} done'.format(epoch))\n",
    "            print(iter_loss, bs_to_check, 1/(2*L_k_1))\n",
    "            epoch_done = False\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
