{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114267b70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets and normalize\n",
    "dataset = 'MNIST'\n",
    "\n",
    "if dataset == 'MNIST':\n",
    "    mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "    mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "    print(len(mnist_trainset))\n",
    "    print(len(mnist_testset))\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for x, y in mnist_trainset:\n",
    "        train_x.append(np.array(x, dtype=np.float32).reshape(1, -1))\n",
    "        train_y.append(y)\n",
    "\n",
    "    train_x = np.array(train_x).reshape(-1, 784)\n",
    "    mean_x = np.mean(train_x, axis=0).reshape(1, -1)\n",
    "    std_x = np.std(train_x)\n",
    "\n",
    "    train_x = (train_x - mean_x) / std_x\n",
    "\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for x, y in mnist_testset:\n",
    "        test_x.append(np.array(x, dtype=np.float32).reshape(1, -1))\n",
    "        test_y.append(y)\n",
    "\n",
    "    test_x = np.array(test_x).reshape(-1, 784)\n",
    "    mean_x = np.mean(test_x, axis=0).reshape(1, -1)\n",
    "    std_x = np.std(test_x)\n",
    "\n",
    "    test_x = (test_x - mean_x) / std_x\n",
    "elif dataset == 'CIFAR10':\n",
    "    cifar_trainset = torchvision.datasets.CIFAR10(root='./data/', train=True, download=True)\n",
    "    cifar_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
    "    print(len(cifar_trainset))\n",
    "    print(len(cifar_testset))\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for x, y in cifar_trainset:\n",
    "        train_x.append(np.array(x, dtype=np.float32).reshape(3, 32, 32))\n",
    "        try:\n",
    "            train_y.append(y.item())\n",
    "        except:\n",
    "            train_y.append(y)\n",
    "\n",
    "    train_x = np.array(train_x).reshape(-1, 3, 32, 32)\n",
    "    mean_x = np.mean(train_x, axis=0)\n",
    "    std_x = np.std(train_x)\n",
    "\n",
    "    train_x = (train_x - mean_x) / std_x\n",
    "\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for x, y in cifar_testset:\n",
    "        test_x.append(np.array(x, dtype=np.float32).reshape(3, 32, 32))\n",
    "        try:\n",
    "            test_y.append(y.item())\n",
    "        except:\n",
    "            test_y.append(y)\n",
    "\n",
    "    test_x = np.array(test_x).reshape(-1, 3, 32, 32)\n",
    "    mean_x = np.mean(test_x, axis=0)\n",
    "    std_x = np.std(test_x)\n",
    "\n",
    "    test_x = (test_x - mean_x) / std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch based on current L_k\n",
    "def get_batch(dataset, train_x, train_y, up_const, L_k, convex, fast=False, alpha=None, bs=False):\n",
    "    global epoch_done\n",
    "    global available_idx\n",
    "    if fast and alpha:\n",
    "        batch_size = max(int(up_const*D_0*alpha / (epsilon)), 1)\n",
    "    else:\n",
    "        if convex:\n",
    "            batch_size = max(int(D_0 / (L_k * epsilon)), 1)\n",
    "        else:\n",
    "            batch_size = max(int(8 * D_0 / (epsilon**2)), 1)\n",
    "    try:\n",
    "        if bs:\n",
    "            batch_size = bs\n",
    "        ids = np.random.choice(available_idx, size=batch_size, replace=False)\n",
    "    except ValueError:\n",
    "        ids = available_idx[:]\n",
    "        epoch_done = True\n",
    "    \n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for n, i in enumerate(ids):\n",
    "        if dataset == 'MNIST':\n",
    "            batch_x.append(train_x[i, :])\n",
    "        elif dataset == 'CIFAR10':\n",
    "            batch_x.append(train_x[i, :, :, :].reshape(1, 3, 32, 32))\n",
    "        batch_y.append(train_y[i])\n",
    "        available_idx.remove(i)\n",
    "    if not available_idx:\n",
    "        epoch_done = True\n",
    "        available_idx = list(range(len(train_y)))\n",
    "    return batch_size, torch.FloatTensor(np.vstack(tuple(batch_x))), torch.LongTensor(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tasks\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 1000, bias=True)\n",
    "        self.linear2 = nn.Linear(1000, 10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class FullyConnectedRELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedRELU, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 1000, bias=True)\n",
    "        self.linear2 = nn.Linear(1000, 10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive method for FullyConnected on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "available_idx = list(range(len(train_x)))\n",
    "epoch_done = False\n",
    "n_epoch = 1000\n",
    "net = FullyConnectedRELU()\n",
    "convex = True # True is for Alg. 2 in paper, False is for Alg. 5 in paper\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "D_0 = 0.1\n",
    "epsilon = 0.002\n",
    "L_k = 1.\n",
    "\n",
    "iter_losses = []\n",
    "iter_train_accs = []\n",
    "iter_test_accs = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print(len(available_idx))\n",
    "    print(epoch_done)\n",
    "    \n",
    "    while True:\n",
    "        bs_to_check, x_to_check, y_to_check = get_batch(dataset, train_x, train_y, 2, L_k, convex)\n",
    "\n",
    "        out = net(x_to_check)\n",
    "        loss = criterion(out, y_to_check)\n",
    "        loss_before = loss\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        i_k = 0\n",
    "        while True:\n",
    "\n",
    "            # params update\n",
    "\n",
    "            L_k = (2**(i_k - 1))*L_k\n",
    "\n",
    "            right_hands = []\n",
    "            for f in net.parameters():\n",
    "                right_hands.append((-1./(8.*L_k))*f.grad.data.norm(2)**2)\n",
    "                f.data.sub_(f.grad.data / (2.*L_k))\n",
    "\n",
    "            right = sum(right_hands)\n",
    "\n",
    "            out = net(x_to_check)\n",
    "            loss_after = criterion(out, y_to_check)\n",
    "\n",
    "            # check condition\n",
    "            if convex:\n",
    "                condition = loss_before + right + epsilon/(2.)\n",
    "            else:\n",
    "                condition = loss_before + right + epsilon**2/(32*L_k)\n",
    "            if loss_after <= condition:\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    out_train = net(torch.from_numpy(train_x))\n",
    "                    out_test = net(torch.from_numpy(test_x))\n",
    "                    iter_losses.append(criterion(out_train, torch.from_numpy(np.array(train_y))).item())\n",
    "\n",
    "                    pred_train = np.argmax(out_train.detach().numpy(), axis=1)\n",
    "                    ground_train = np.array(train_y)\n",
    "                    l = float(len(ground_train))\n",
    "                    iter_train_accs.append(len(np.where(pred_train == ground_train)[0]) / l)\n",
    "\n",
    "                    pred_test = np.argmax(out_test.detach().numpy(),axis=1)\n",
    "                    ground_test = np.array(test_y)\n",
    "                    l = float(len(ground_test))\n",
    "                    iter_test_accs.append(len(np.where(pred_test == ground_test)[0]) / l)\n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                for f in net.parameters():\n",
    "                    f.data.add_(f.grad.data / (2.*L_k))\n",
    "\n",
    "                i_k += 1\n",
    "        \n",
    "        if epoch_done:\n",
    "            print('epoch {} done'.format(epoch))\n",
    "            print(len(iter_losses))\n",
    "            print(iter_losses[-10:])\n",
    "            epoch_done = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Adaptive method for FullyConnected on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alg 3 in paper\n",
    "available_idx = list(range(len(mnist_trainset)))\n",
    "epoch_done = False\n",
    "n_epoch = 1000\n",
    "net = FullyConnectedRELU()\n",
    "convex = False\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "iter_losses = []\n",
    "iter_train_accs = []\n",
    "iter_test_accs = []\n",
    "\n",
    "D_0 = 0.1\n",
    "epsilon = 0.002\n",
    "L_old = 1.\n",
    "\n",
    "x_old = list(net.parameters())\n",
    "y_old = list(net.parameters())\n",
    "u_old = list(net.parameters())\n",
    "alpha_old = 0.\n",
    "A_old = 0.\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print(len(available_idx))\n",
    "    print(epoch_done)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        i_k = 0\n",
    "        alpha_batch = (1 + np.sqrt(1 + 4*A_old*L_old)) / (2*L_old)\n",
    "        bs_to_check, x_to_check, y_to_check = get_batch(dataset, train_x, train_y, 3., False, convex, True, alpha_batch)\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            L_k = (2**(i_k - 1))*L_old\n",
    "            alpha_k = (1 + np.sqrt(1 + 4*A_old*L_k)) / (2*L_k)\n",
    "            A_k = A_old + alpha_k\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_k = []\n",
    "                for u, x, param in zip(u_old, x_old, list(net.parameters())):\n",
    "                    new_value = (alpha_k*u + A_old*x) / A_k\n",
    "                    y_k.append(new_value)\n",
    "                    param.data = new_value\n",
    "\n",
    "            net.zero_grad()\n",
    "            out = net(x_to_check)\n",
    "            loss_y_k = criterion(out, y_to_check)\n",
    "            loss_y_k.backward()\n",
    "\n",
    "            dot_prods = []\n",
    "            norms = []\n",
    "            u_k = []\n",
    "            x_k = []\n",
    "            for x, u, y_k_value, param in zip(x_old, u_old, y_k, list(net.parameters())):\n",
    "                u_k_value = u - alpha_k*param.grad.data\n",
    "                u_k.append(u_k_value)\n",
    "                x_k_value = (alpha_k*u_k_value + A_old*x) / A_k\n",
    "                x_k.append(x_k_value)\n",
    "\n",
    "                dot_prods.append(torch.sum((param.grad.data*(x_k_value-y_k_value)).view(-1)))\n",
    "                norms.append(torch.sum(((x_k_value-y_k_value)*(x_k_value-y_k_value)).view(-1)))\n",
    "\n",
    "                param.data = x_k_value\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                out = net(x_to_check)\n",
    "                loss_x_k = criterion(out, y_to_check)\n",
    "                \n",
    "            i_k += 1    \n",
    "            if loss_x_k <= loss_y_k + sum(dot_prods) + (L_k/2.)*sum(norms) + epsilon/(L_k*alpha_k):\n",
    "                alpha_old = alpha_k\n",
    "                A_old = A_k\n",
    "                u_old = u_k\n",
    "                x_old = x_k\n",
    "                L_old = L_k\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    out_train = net(torch.from_numpy(train_x))\n",
    "                    out_test = net(torch.from_numpy(test_x))\n",
    "                    iter_losses.append(criterion(out_train, torch.from_numpy(np.array(train_y))).item())\n",
    "\n",
    "                    pred_train = np.argmax(out_train.detach().numpy(), axis=1)\n",
    "                    ground_train = np.array(train_y)\n",
    "                    l = float(len(ground_train))\n",
    "                    iter_train_accs.append(len(np.where(pred_train == ground_train)[0]) / l)\n",
    "\n",
    "                    pred_test = np.argmax(out_test.detach().numpy(),axis=1)\n",
    "                    ground_test = np.array(test_y)\n",
    "                    l = float(len(ground_test))\n",
    "                    iter_test_accs.append(len(np.where(pred_test == ground_test)[0]) / l)\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                for x, f in zip(x_old, list(net.parameters())):\n",
    "                    f.data = x        \n",
    "        \n",
    "        if epoch_done:\n",
    "            print('epoch {} done'.format(epoch))\n",
    "            print(len(iter_losses))\n",
    "            epoch_done = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam or Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_idx = list(range(len(train_x)))\n",
    "epoch_done = False\n",
    "n_epoch = 10\n",
    "net = FullyConnectedRELU()\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "iter_losses = []\n",
    "iter_train_accs = []\n",
    "iter_test_accs = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print(len(available_idx))\n",
    "    print(epoch_done)\n",
    "    \n",
    "    while True:\n",
    "        optimizer.zero_grad()\n",
    "        bs_to_check, x_to_check, y_to_check = get_batch(dataset, train_x, train_y, 2, L_k, False, bs=128)\n",
    "        out = net(x_to_check)\n",
    "        loss = criterion(out, y_to_check)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            out_train = net(torch.from_numpy(train_x))\n",
    "            out_test = net(torch.from_numpy(test_x))\n",
    "            iter_losses.append(criterion(out_train, torch.from_numpy(np.array(train_y))).item())\n",
    "\n",
    "            pred_train = np.argmax(out_train.detach().numpy(), axis=1)\n",
    "            ground_train = np.array(train_y)\n",
    "            l = float(len(ground_train))\n",
    "            iter_train_accs.append(len(np.where(pred_train == ground_train)[0]) / l)\n",
    "\n",
    "            pred_test = np.argmax(out_test.detach().numpy(),axis=1)\n",
    "            ground_test = np.array(test_y)\n",
    "            l = float(len(ground_test))\n",
    "            iter_test_accs.append(len(np.where(pred_test == ground_test)[0]) / l)\n",
    "            \n",
    "        if epoch_done:\n",
    "            print('epoch {} done'.format(epoch))\n",
    "            print(len(iter_losses))\n",
    "            epoch_done = False\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
